{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-16T16:23:46.021215Z",
     "iopub.status.busy": "2025-08-16T16:23:46.020663Z",
     "iopub.status.idle": "2025-08-16T16:23:46.024851Z",
     "shell.execute_reply": "2025-08-16T16:23:46.024231Z",
     "shell.execute_reply.started": "2025-08-16T16:23:46.021192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T16:23:46.043978Z",
     "iopub.status.busy": "2025-08-16T16:23:46.043727Z",
     "iopub.status.idle": "2025-08-16T16:23:46.058231Z",
     "shell.execute_reply": "2025-08-16T16:23:46.057621Z",
     "shell.execute_reply.started": "2025-08-16T16:23:46.043961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"your_huggingface_token_here\"  # Replace with your Hugging Face token\n",
    "MODEL_ID = \"Viet-Mistral/Vistral-7B-Chat\"\n",
    "INPUT_PATH = \"/kaggle/input/vlsp-dataset/public_test.txt\"\n",
    "OUTPUT_PATH = \"/kaggle/working/vistral_predictions.txt\"\n",
    "MAX_NEW_TOKENS = 64\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade bitsandbytes\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T16:25:39.873409Z",
     "iopub.status.busy": "2025-08-16T16:25:39.872658Z",
     "iopub.status.idle": "2025-08-16T16:26:00.344425Z",
     "shell.execute_reply": "2025-08-16T16:26:00.343501Z",
     "shell.execute_reply.started": "2025-08-16T16:25:39.873378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Đăng nhập Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0024b31a37e349938bc21cee597cb669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "!pip install -U bitsandbytes\n",
    "# ====== Auth + Load Model ======\n",
    "print(\"Đăng nhập Hugging Face...\")\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# ====== Quantization config ======\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # hoặc load_in_4bit=True nếu muốn 4-bit\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # dtype tính toán\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# ====== Tokenizer ======\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)\n",
    "\n",
    "# ====== Load model với quantization ======\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",        # auto phân phối GPU/CPU\n",
    "    offload_folder=\"/tmp\"     # offload layer chưa dùng sang CPU\n",
    ")\n",
    "\n",
    "# ====== Pipeline ======\n",
    "chat_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T16:26:26.756248Z",
     "iopub.status.busy": "2025-08-16T16:26:26.755498Z",
     "iopub.status.idle": "2025-08-16T16:26:26.785877Z",
     "shell.execute_reply": "2025-08-16T16:26:26.785162Z",
     "shell.execute_reply.started": "2025-08-16T16:26:26.756215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc dữ liệu từ: /kaggle/input/vlsp-dataset/public_test.txt\n",
      "Số lượng record: 400\n"
     ]
    }
   ],
   "source": [
    "# ====== Load Input Data ======\n",
    "print(f\"Đọc dữ liệu từ: {INPUT_PATH}\")\n",
    "with open(INPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "print(f\"Số lượng record: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T16:41:39.249444Z",
     "iopub.status.busy": "2025-08-16T16:41:39.248629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang suy luận...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/400 [01:24<9:23:11, 84.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      " 10%|█         | 40/400 [59:25<10:15:27, 102.58s/it]"
     ]
    }
   ],
   "source": [
    "# ====== Inference & Ghi file ======\n",
    "def classify_option(context, question, option):\n",
    "    prompt = f\"\"\"\n",
    "Bạn là trợ lý AI, nhiệm vụ phân loại các phương án trả lời về thời lượng:\n",
    "- Input là JSON có fields \"context\", \"question\", \"options\" (là list các string)\n",
    "- Output chỉ trả về JSON list \"labels\" có cùng độ dài với \"options\", giá trị \"yes\" nếu option đúng mô tả thời lượng, ngược lại \"no\".\n",
    "- Các phương án trả lời có thể là thời gian thực tế hoặc ước lượng, không cần chính xác tuyệt đối.\n",
    "- Trong mỗi question, không được phép gán tất cả các labels là \"yes\" hoặc \"no\".\n",
    "\n",
    "Bây giờ phân loại cho record sau:\n",
    "{json.dumps(record, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = chat_pipeline(prompt)[0][\"generated_text\"]\n",
    "        for line in output.split('\\n')[::-1]:\n",
    "            if \"yes\" in line.lower():\n",
    "                return \"yes\"\n",
    "            if \"no\" in line.lower():\n",
    "                return \"no\"\n",
    "    except Exception as e:\n",
    "        print(f\"[Lỗi mô hình] {e}\")\n",
    "    return \"no\"\n",
    "\n",
    "print(\"Đang suy luận...\")\n",
    "output_data = []\n",
    "\n",
    "for ex in tqdm(data):\n",
    "    preds = []\n",
    "    for opt in ex[\"options\"]:\n",
    "        pred = classify_option(ex[\"context\"], ex[\"question\"], opt)\n",
    "        preds.append(pred)\n",
    "\n",
    "    record_out = {\n",
    "        \"context\": ex[\"context\"],\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"options\": ex[\"options\"],\n",
    "        \"qid\": ex[\"qid\"],\n",
    "        \"labels\": ex.get(\"labels\", []),  # fallback nếu không có\n",
    "        \"predictions\": preds\n",
    "    }\n",
    "\n",
    "    output_data.append(record_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Save Output ======\n",
    "OUTPUT_PATH = \"/kaggle/working/vistral_predictions.txt\"\n",
    "\n",
    "print(f\"Ghi kết quả ra: {OUTPUT_PATH}\")\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in output_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Hoàn tất!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# liệt kê tất cả file trong /kaggle/working/\n",
    "print(os.listdir(\"/kaggle/working/\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8079427,
     "sourceId": 12779636,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
